{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primavera Companion: Make your personalized Primavera Sound schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Spotify has removed the endpoint to access the audio attributes, we obtained these values via an external tool manually and placed the required csv files in the \"data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Data Loading and Initial Exploration -----\n",
    "def load_and_explore_data():\n",
    "    \"\"\"Load the data files and perform initial exploration\"\"\"\n",
    "    # Check if data exists in expected location\n",
    "    data_dir = \"./data\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Warning: {data_dir} directory not found. Using current directory.\")\n",
    "        data_dir = \".\"\n",
    "    \n",
    "    # Find CSV files - you might need to adjust filenames\n",
    "    my_playlist_path = None\n",
    "    primavera_playlist_path = None\n",
    "    \n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.csv'):\n",
    "            if 'my' in file.lower() or 'personal' in file.lower() or 'taste' in file.lower() or 'matrix' in file.lower():\n",
    "                my_playlist_path = os.path.join(data_dir, file)\n",
    "            elif 'primavera' in file.lower() or 'festival' in file.lower():\n",
    "                primavera_playlist_path = os.path.join(data_dir, file)\n",
    "    \n",
    "    if not my_playlist_path:\n",
    "        my_playlist_path = input(\"Enter the path to your personal playlist CSV: \")\n",
    "    if not primavera_playlist_path:\n",
    "        primavera_playlist_path = input(\"Enter the path to the Primavera playlist CSV: \")\n",
    "    \n",
    "    # Load datasets\n",
    "    print(f\"Loading personal playlist from: {my_playlist_path}\")\n",
    "    my_playlist = pd.read_csv(my_playlist_path)\n",
    "    \n",
    "    print(f\"Loading Primavera playlist from: {primavera_playlist_path}\")\n",
    "    primavera_playlist = pd.read_csv(primavera_playlist_path)\n",
    "    \n",
    "    # Basic exploration\n",
    "    print(\"\\n----- Personal Playlist Summary -----\")\n",
    "    print(f\"Number of tracks: {len(my_playlist)}\")\n",
    "    print(f\"Number of unique artists: {my_playlist['Artist Name(s)'].str.split(', ').explode().nunique()}\")\n",
    "    print(f\"Columns available: {my_playlist.columns.tolist()}\")\n",
    "    \n",
    "    print(\"\\n----- Primavera Playlist Summary -----\")\n",
    "    print(f\"Number of tracks: {len(primavera_playlist)}\")\n",
    "    print(f\"Number of unique artists: {primavera_playlist['Artist Name(s)'].str.split(', ').explode().nunique()}\")\n",
    "    print(f\"Columns available: {primavera_playlist.columns.tolist()}\")\n",
    "    \n",
    "    return my_playlist, primavera_playlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Data Preprocessing with Featured Artist Handling -----\n",
    "def preprocess_playlist_data(playlist_df, is_training=True, shared_genres=None, return_genres=False, min_artist_frequency=None):\n",
    "    \"\"\"\n",
    "    Preprocess playlist data for artist-level aggregation.\n",
    "    - Explode artist collaborations into separate rows\n",
    "    - Extract and encode genres\n",
    "    - Aggregate numerical features by artist\n",
    "    - Filter out infrequent artists (for Primavera data)\n",
    "    \n",
    "    Args:\n",
    "        playlist_df: DataFrame containing playlist data\n",
    "        is_training: Whether this is training data (True) or test data (False)\n",
    "        shared_genres: List of shared genres to use (for test data)\n",
    "        return_genres: Whether to return genre frequency info (True for initial training)\n",
    "        min_artist_frequency: Minimum frequency for artists to be included (None = no filtering)\n",
    "    \"\"\"\n",
    "    print(f\"\\n----- Preprocessing {'training' if is_training else 'test'} data -----\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df = playlist_df.copy()\n",
    "    \n",
    "    # Check for needed columns and handle missing ones\n",
    "    required_columns = [\n",
    "        'Artist Name(s)', 'Genres', 'Popularity', 'Danceability', 'Energy', \n",
    "        'Key', 'Loudness', 'Mode', 'Speechiness', 'Acousticness', \n",
    "        'Instrumentalness', 'Liveness', 'Valence', 'Tempo', 'Time Signature'\n",
    "    ]\n",
    "    \n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found. Adding dummy column.\")\n",
    "            if col in ['Key', 'Mode', 'Time Signature']:\n",
    "                df[col] = 0  # Default for categorical\n",
    "            else:\n",
    "                df[col] = np.nan  # Default for numerical\n",
    "    \n",
    "    # Handle missing values in numeric columns\n",
    "    numeric_cols = [\n",
    "        'Popularity', 'Danceability', 'Energy', 'Loudness', \n",
    "        'Speechiness', 'Acousticness', 'Instrumentalness', \n",
    "        'Liveness', 'Valence', 'Tempo'\n",
    "    ]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if df[col].isna().any():\n",
    "            median_val = df[col].median()\n",
    "            print(f\"Filling {df[col].isna().sum()} missing values in '{col}' with median: {median_val:.2f}\")\n",
    "            df[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    # Convert categorical columns to integers to avoid type issues\n",
    "    for col in ['Key', 'Mode', 'Time Signature']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Explode artists to create one row per artist\n",
    "    print(\"Exploding multiple artists in collaborations...\")\n",
    "    df['Artist Name(s)'] = df['Artist Name(s)'].astype(str)\n",
    "    \n",
    "    # Handle different separators for artist names (comma or comma+space)\n",
    "    artist_lists = []\n",
    "    for artists_str in df['Artist Name(s)']:\n",
    "        if pd.isna(artists_str) or artists_str == '' or artists_str.lower() == 'nan':\n",
    "            artist_lists.append([])\n",
    "        else:\n",
    "            if ', ' in artists_str:\n",
    "                artist_lists.append([a.strip() for a in artists_str.split(', ') if a.strip()])\n",
    "            else:\n",
    "                artist_lists.append([a.strip() for a in artists_str.split(',') if a.strip()])\n",
    "    \n",
    "    df['Artist_List'] = artist_lists\n",
    "    \n",
    "    # Explode the dataframe on the artist list\n",
    "    exploded_artists = df.explode('Artist_List')\n",
    "    \n",
    "    # Rename and clean up\n",
    "    exploded_artists.rename(columns={'Artist_List': 'Artist'}, inplace=True)\n",
    "    \n",
    "    # Handle empty or NaN artist names\n",
    "    exploded_artists = exploded_artists[exploded_artists['Artist'].notna() & (exploded_artists['Artist'] != '')]\n",
    "    \n",
    "    # Filter out infrequent artists if min_artist_frequency is specified (for Primavera data)\n",
    "    if min_artist_frequency is not None and min_artist_frequency > 0:\n",
    "        artist_counts = exploded_artists['Artist'].value_counts()\n",
    "        frequent_artists = artist_counts[artist_counts >= min_artist_frequency].index\n",
    "        \n",
    "        original_count = exploded_artists['Artist'].nunique()\n",
    "        exploded_artists = exploded_artists[exploded_artists['Artist'].isin(frequent_artists)]\n",
    "        \n",
    "        new_count = exploded_artists['Artist'].nunique()\n",
    "        print(f\"Filtered out {original_count - new_count} infrequent artists\")\n",
    "        print(f\"Keeping {new_count} main artists with at least {min_artist_frequency} tracks\")\n",
    "        \n",
    "        # If we filtered out all artists, that's a problem\n",
    "        if len(exploded_artists) == 0:\n",
    "            print(\"WARNING: All artists were filtered out. Reducing min_artist_frequency.\")\n",
    "            # Try with a lower threshold\n",
    "            min_artist_frequency = 1\n",
    "            exploded_artists = df.explode('Artist_List')\n",
    "            exploded_artists.rename(columns={'Artist_List': 'Artist'}, inplace=True)\n",
    "            exploded_artists = exploded_artists[exploded_artists['Artist'].notna() & (exploded_artists['Artist'] != '')]\n",
    "    \n",
    "    # Process genres - improved handling for different formats\n",
    "    print(\"Processing genres...\")\n",
    "    \n",
    "    # Check if Genres column exists, create it if it doesn't\n",
    "    if 'Genres' not in exploded_artists.columns:\n",
    "        print(\"Warning: 'Genres' column not found. Creating empty column.\")\n",
    "        exploded_artists['Genres'] = ''\n",
    "    \n",
    "    # Ensure Genres column is string type\n",
    "    exploded_artists['Genres'] = exploded_artists['Genres'].astype(str)\n",
    "    \n",
    "    # Handle different genre separators (comma or comma+space)\n",
    "    # Some may use ',', others may use ', '\n",
    "    genre_lists = []\n",
    "    for genres_str in exploded_artists['Genres']:\n",
    "        # Handle NaN, empty strings, and 'nan' string\n",
    "        if pd.isna(genres_str) or genres_str == '' or genres_str.lower() == 'nan':\n",
    "            genre_lists.append([])\n",
    "        else:\n",
    "            # Try splitting by comma and space, if that doesn't work, try just comma\n",
    "            if ', ' in genres_str:\n",
    "                genre_lists.append([g.strip() for g in genres_str.split(', ') if g.strip()])\n",
    "            else:\n",
    "                genre_lists.append([g.strip() for g in genres_str.split(',') if g.strip()])\n",
    "    \n",
    "    exploded_artists['Genre_List'] = genre_lists\n",
    "    \n",
    "    # Calculate genre frequencies for training data\n",
    "    if is_training and return_genres:\n",
    "        # Flatten all genre lists\n",
    "        all_genres_flat = [genre for sublist in genre_lists for genre in sublist if genre]\n",
    "        \n",
    "        # Count genre frequencies\n",
    "        genre_counts = pd.Series(all_genres_flat).value_counts()\n",
    "        \n",
    "        print(f\"Found {len(genre_counts)} unique genres in training data\")\n",
    "        print(f\"Top 5 genres: {genre_counts.head(5).to_dict()}\")\n",
    "    \n",
    "    # If shared_genres is provided, use only those genres\n",
    "    # Otherwise, use all genres if in training mode\n",
    "    if shared_genres is not None:\n",
    "        genres_to_use = shared_genres\n",
    "        print(f\"Using {len(genres_to_use)} shared genres from both datasets\")\n",
    "    elif is_training:\n",
    "        # Extract all unique genres\n",
    "        all_genres = set()\n",
    "        for genre_list in genre_lists:\n",
    "            all_genres.update(genre_list)\n",
    "        \n",
    "        # Remove any empty strings\n",
    "        if '' in all_genres:\n",
    "            all_genres.remove('')\n",
    "        \n",
    "        genres_to_use = all_genres\n",
    "        print(f\"Using all {len(genres_to_use)} unique genres in training data\")\n",
    "    else:\n",
    "        # This should not happen if code is used correctly\n",
    "        print(\"WARNING: No shared_genres provided for test data. Using empty set.\")\n",
    "        genres_to_use = set()\n",
    "    \n",
    "    # Create dummy variables for genres (one-hot encoding)\n",
    "    for genre in genres_to_use:\n",
    "        exploded_artists[f'Genre_{genre}'] = exploded_artists['Genre_List'].apply(\n",
    "            lambda x: 1 if genre in x else 0\n",
    "        )\n",
    "    \n",
    "    # Drop the temporary Genre_List column\n",
    "    exploded_artists.drop('Genre_List', axis=1, inplace=True)\n",
    "    \n",
    "    # Count tracks per artist (for training data)\n",
    "    if is_training:\n",
    "        print(\"Calculating artist frequency in personal playlist...\")\n",
    "        artist_frequency = exploded_artists['Artist'].value_counts().reset_index()\n",
    "        artist_frequency.columns = ['Artist', 'Track_Count']\n",
    "    \n",
    "    # Aggregate numerical features by artist\n",
    "    print(\"Aggregating features by artist...\")\n",
    "    \n",
    "    # List of numerical features to aggregate\n",
    "    num_features = [\n",
    "        'Popularity', 'Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "        'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo'\n",
    "    ]\n",
    "    \n",
    "    # Create aggregation dictionary\n",
    "    agg_dict = {}\n",
    "    for feature in num_features:\n",
    "        agg_dict[feature] = ['min', 'max', 'mean', 'var']\n",
    "    \n",
    "    # Add genre columns to aggregation (using max because they're 0/1)\n",
    "    genre_cols = [col for col in exploded_artists.columns if col.startswith('Genre_')]\n",
    "    for col in genre_cols:\n",
    "        agg_dict[col] = 'max'  # Just use 'max' without renaming\n",
    "        \n",
    "    # Add categorical columns with mode aggregation\n",
    "    for col in ['Key', 'Mode', 'Time Signature']:\n",
    "        agg_dict[col] = lambda x: x.mode().iloc[0] if not x.empty and len(x.mode()) > 0 else 0\n",
    "    \n",
    "    # Perform aggregation\n",
    "    artist_features = exploded_artists.groupby('Artist').agg(agg_dict)\n",
    "    \n",
    "    # Flatten multi-level column names, but preserve genre column names WITHOUT _max suffix\n",
    "    new_columns = []\n",
    "    for col in artist_features.columns:\n",
    "        if col[0].startswith('Genre_') and col[1] == 'max':\n",
    "            # For genre columns, just use the original name without the _max suffix\n",
    "            new_columns.append(col[0])\n",
    "        else:\n",
    "            # For other columns, join the names\n",
    "            new_columns.append('_'.join(col).strip())\n",
    "    \n",
    "    artist_features.columns = new_columns\n",
    "    \n",
    "    # Reset index to make 'Artist' a column\n",
    "    artist_features.reset_index(inplace=True)\n",
    "    \n",
    "    # For training data, merge with track count\n",
    "    if is_training:\n",
    "        artist_features = artist_features.merge(artist_frequency, on='Artist', how='left')\n",
    "        artist_features['Track_Count'].fillna(0, inplace=True)\n",
    "        print(f\"Created features for {len(artist_features)} artists with track count as target variable\")\n",
    "    else:\n",
    "        print(f\"Created features for {len(artist_features)} artists\")\n",
    "    \n",
    "    # Return the processed data, and genre counts if requested\n",
    "    if is_training and return_genres:\n",
    "        return artist_features, genre_counts\n",
    "    return artist_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Model Training and Evaluation -----\n",
    "def train_and_evaluate_models(train_data):\n",
    "    \"\"\"Train multiple regression models and evaluate their performance\"\"\"\n",
    "    print(\"\\n----- Training and Evaluating Models -----\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = train_data.drop(['Artist', 'Track_Count'], axis=1)\n",
    "    y = train_data['Track_Count']\n",
    "    \n",
    "    # Fill any remaining NaN values\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    # Initialize models to try\n",
    "    models = {\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Lasso Regression': Lasso(alpha=0.1),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf')\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Train with cross-validation\n",
    "        cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "        rmse_scores = np.sqrt(-cv_scores)\n",
    "        \n",
    "        # Train on full dataset for later prediction\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'cv_rmse_mean': rmse_scores.mean(),\n",
    "            'cv_rmse_std': rmse_scores.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"  {name} - RMSE: {rmse_scores.mean():.4f} (±{rmse_scores.std():.4f})\")\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = min(results, key=lambda k: results[k]['cv_rmse_mean'])\n",
    "    print(f\"\\nBest performing model: {best_model_name} with RMSE: {results[best_model_name]['cv_rmse_mean']:.4f}\")\n",
    "    \n",
    "    # Feature importance for tree-based models\n",
    "    if 'Random Forest' in results:\n",
    "        rf_model = results['Random Forest']['model']\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Importance': rf_model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 most important features (Random Forest):\")\n",
    "        print(feature_importance.head(10))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Prediction and Ranking -----\n",
    "def predict_and_rank_artists(test_data, model_results):\n",
    "    \"\"\"Predict scores for test artists and rank them\"\"\"\n",
    "    print(\"\\n----- Predicting and Ranking Artists -----\")\n",
    "    \n",
    "    # Prepare test features\n",
    "    X_test = test_data.drop(['Artist'], axis=1)\n",
    "    \n",
    "    # Fill any remaining NaN values\n",
    "    X_test = X_test.fillna(0)\n",
    "    \n",
    "    # Use the best model for prediction\n",
    "    best_model_name = min(model_results, key=lambda k: model_results[k]['cv_rmse_mean'])\n",
    "    best_model = model_results[best_model_name]['model']\n",
    "    \n",
    "    print(f\"Using {best_model_name} for prediction...\")\n",
    "    \n",
    "    # Predict scores\n",
    "    test_data_copy = test_data.copy()\n",
    "    test_data_copy['Predicted_Score'] = best_model.predict(X_test)\n",
    "    \n",
    "    # Ensure no negative scores\n",
    "    test_data_copy['Predicted_Score'] = test_data_copy['Predicted_Score'].clip(lower=0)\n",
    "    \n",
    "    # Rank artists by predicted score\n",
    "    ranked_artists = test_data_copy[['Artist', 'Predicted_Score']].sort_values(\n",
    "        'Predicted_Score', ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Add rank column\n",
    "    ranked_artists['Rank'] = ranked_artists.index + 1\n",
    "    \n",
    "    # Reorder columns\n",
    "    ranked_artists = ranked_artists[['Rank', 'Artist', 'Predicted_Score']]\n",
    "    \n",
    "    print(\"\\nTop 20 recommended artists from Primavera:\")\n",
    "    print(ranked_artists.head(20))\n",
    "    \n",
    "    return ranked_artists, test_data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Artist Overlap Analysis -----\n",
    "def analyze_artist_overlap(ranked_artists, my_playlist_df, primavera_playlist_df):\n",
    "    \"\"\"Analyze overlap between personal playlist and Primavera artists\"\"\"\n",
    "    print(\"\\n----- Analyzing Artist Overlap -----\")\n",
    "    \n",
    "    # Extract all artists from personal playlist\n",
    "    my_artists = set(my_playlist_df['Artist Name(s)'].str.split(', ').explode().unique())\n",
    "    \n",
    "    # Check which Primavera artists are in my playlist\n",
    "    ranked_artists['In_My_Playlist'] = ranked_artists['Artist'].apply(\n",
    "        lambda x: 1 if x in my_artists else 0\n",
    "    )\n",
    "    \n",
    "    # Count overlap\n",
    "    overlap_count = ranked_artists['In_My_Playlist'].sum()\n",
    "    print(f\"Found {overlap_count} Primavera artists in your personal playlist\")\n",
    "    \n",
    "    # Highlight artists in common\n",
    "    if overlap_count > 0:\n",
    "        print(\"\\nPrimavera artists in your playlist:\")\n",
    "        overlap_artists = ranked_artists[ranked_artists['In_My_Playlist'] == 1]\n",
    "        print(overlap_artists[['Rank', 'Artist', 'Predicted_Score']])\n",
    "    \n",
    "    # Adjust scores based on overlap\n",
    "    if overlap_count > 0:\n",
    "        # Find maximum predicted score for normalization\n",
    "        max_score = ranked_artists['Predicted_Score'].max()\n",
    "        \n",
    "        # Apply weight to overlap (0.5 means 50% of the score comes from model, 50% from playlist overlap)\n",
    "        overlap_weight = 0.3  # You can adjust this\n",
    "        \n",
    "        print(f\"\\nAdjusting scores with overlap weight of {overlap_weight:.1f}\")\n",
    "        \n",
    "        # Create adjusted score\n",
    "        ranked_artists['Adjusted_Score'] = (\n",
    "            (1 - overlap_weight) * ranked_artists['Predicted_Score'] + \n",
    "            overlap_weight * max_score * ranked_artists['In_My_Playlist']\n",
    "        )\n",
    "        \n",
    "        # Re-rank based on adjusted score\n",
    "        ranked_artists = ranked_artists.sort_values(\n",
    "            'Adjusted_Score', ascending=False\n",
    "        ).reset_index(drop=True)\n",
    "        \n",
    "        # Update rank\n",
    "        ranked_artists['Adjusted_Rank'] = ranked_artists.index + 1\n",
    "        \n",
    "        print(\"\\nTop 20 artists after adjustment:\")\n",
    "        print(ranked_artists[['Adjusted_Rank', 'Artist', 'Predicted_Score', \n",
    "                             'In_My_Playlist', 'Adjusted_Score']].head(20))\n",
    "    \n",
    "    return ranked_artists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"===== Primavera Sound Artist Recommendation System =====\")\n",
    "\n",
    "# Step 1: Load and explore data\n",
    "my_playlist, primavera_playlist = load_and_explore_data()\n",
    "\n",
    "# Step 2: First pass of data processing to get genres from both datasets\n",
    "print(\"\\n----- First pass: Analyzing genres in both datasets -----\")\n",
    "_, my_genres = preprocess_playlist_data(my_playlist, is_training=True, return_genres=True)\n",
    "\n",
    "# Do a light processing of Primavera data to get genres\n",
    "primavera_exploded = primavera_playlist.copy()\n",
    "primavera_exploded['Genres'] = primavera_exploded['Genres'].astype(str)\n",
    "\n",
    "# Extract genres from Primavera data\n",
    "primavera_genre_lists = []\n",
    "for genres_str in primavera_exploded['Genres']:\n",
    "    if pd.isna(genres_str) or genres_str == '' or genres_str.lower() == 'nan':\n",
    "        primavera_genre_lists.append([])\n",
    "    else:\n",
    "        if ', ' in genres_str:\n",
    "            primavera_genre_lists.append([g.strip() for g in genres_str.split(', ') if g.strip()])\n",
    "        else:\n",
    "            primavera_genre_lists.append([g.strip() for g in genres_str.split(',') if g.strip()])\n",
    "\n",
    "# Flatten all genre lists from Primavera\n",
    "primavera_genres_flat = [genre for sublist in primavera_genre_lists for genre in sublist if genre]\n",
    "\n",
    "# Count genre frequencies in Primavera data\n",
    "primavera_genre_counts = pd.Series(primavera_genres_flat).value_counts()\n",
    "\n",
    "# Find genres that appear in both datasets\n",
    "my_genre_set = set(my_genres.index)\n",
    "primavera_genre_set = set(primavera_genre_counts.index)\n",
    "shared_genres = my_genre_set.intersection(primavera_genre_set)\n",
    "\n",
    "print(f\"Found {len(shared_genres)} genres shared between datasets\")\n",
    "\n",
    "# Get the top 20 most frequent genres from your playlist that also appear in Primavera data\n",
    "top_shared_genres = my_genres[my_genres.index.isin(shared_genres)].nlargest(20).index.tolist()\n",
    "\n",
    "print(f\"Selected top 20 shared genres: {top_shared_genres}\")\n",
    "\n",
    "# Step 3: Actual data preprocessing with selected genres\n",
    "# For personal playlist - no minimum frequency filtering\n",
    "train_data = preprocess_playlist_data(my_playlist, is_training=True, shared_genres=top_shared_genres)\n",
    "\n",
    "# For Primavera playlist - apply minimum frequency filtering to remove featured artists\n",
    "# Use min_artist_frequency=5 to filter out artists with fewer than 5 tracks (likely features)\n",
    "test_data = preprocess_playlist_data(\n",
    "    primavera_playlist, \n",
    "    is_training=False, \n",
    "    shared_genres=top_shared_genres,\n",
    "    min_artist_frequency=5\n",
    ")\n",
    "\n",
    "# Save processed data\n",
    "train_data.to_csv('processed_training_data.csv', index=False)\n",
    "test_data.to_csv('processed_test_data.csv', index=False)\n",
    "print(\"Saved processed data as CSV files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train and evaluate models\n",
    "model_results = train_and_evaluate_models(train_data)\n",
    "\n",
    "# Step 5: Predict and rank artists\n",
    "ranked_artists, predicted_test_data = predict_and_rank_artists(test_data, model_results)\n",
    "\n",
    "# Step 6: Analyze artist overlap\n",
    "ranked_artists = analyze_artist_overlap(\n",
    "    ranked_artists, my_playlist, primavera_playlist\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
